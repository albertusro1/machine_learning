{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Latihan Standardization.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNJaId4KAFAmMwmjwsunNNJ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_fg_AXgxcZId","executionInfo":{"status":"ok","timestamp":1601985182071,"user_tz":-420,"elapsed":663,"user":{"displayName":"Rowan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2gAEweV7GeAjWGYfH9tbebwqtaKwR7fTmx1sJ=s64","userId":"13363543679705957531"}},"outputId":"ab308b7b-3610-43b6-aceb-08e201b09f02","colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["# import preprocessing dari sklearn\n","from sklearn import preprocessing\n","\n","# input the data\n","data = [[12000000, 33], [35000000, 45], [4000000, 23], [6500000, 26], [9000000, 29]]\n","\n","# create object scaler to fit the data\n","scaler = preprocessing.StandardScaler().fit(data)\n","\n","# transform data (standarize)\n","data = scaler.transform(data)\n","\n","# print the data\n","print(data)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[[-0.11638732  0.23521877]\n"," [ 1.94277296  1.80334389]\n"," [-0.83261698 -1.07155217]\n"," [-0.60879521 -0.67952089]\n"," [-0.38497344 -0.28748961]]\n"],"name":"stdout"}]}]}